{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with parquet files\n",
    "\n",
    "## Objective\n",
    "\n",
    "+ In this assignment, we will use the data downloaded with the module `data_manager` to create features.\n",
    "\n",
    "(11 pts total)\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "+ This notebook assumes that price data is available to you in the environment variable `PRICE_DATA`. If you have not done so, then execute the notebook `01_materials/labs/2_data_engineering.ipynb` to create this data set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Load the environment variables using dotenv. (1 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code below.\n",
    "%load_ext dotenv\n",
    "%dotenv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Load the environment variable `PRICE_DATA`.\n",
    "+ Use [glob](https://docs.python.org/3/library/glob.html) to find the path of all parquet files in the directory `PRICE_DATA`.\n",
    "\n",
    "(1pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# Load the environment variable `PRICE_DATA`\n",
    "PRICE_DATA = os.getenv('PRICE_DATA')\n",
    "\n",
    "# Use [glob](https://docs.python.org/3/library/glob.html) to find the path of all parquet files in the directory `PRICE_DATA`.\n",
    "parquet_files = glob(os.path.join(PRICE_DATA, \"**/*.parquet\"), recursive = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each ticker and using Dask, do the following:\n",
    "\n",
    "+ Add lags for variables Close and Adj_Close.\n",
    "+ Add returns based on Close:\n",
    "    \n",
    "    - `returns`: (Close / Close_lag_1) - 1\n",
    "\n",
    "+ Add the following range: \n",
    "\n",
    "    - `hi_lo_range`: this is the day's High minus Low.\n",
    "\n",
    "+ Assign the result to `dd_feat`.\n",
    "\n",
    "(4 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\The Winner\\AppData\\Local\\Temp\\ipykernel_11120\\159928415.py:4: UserWarning: `meta` is not specified, inferred from partial data. Please provide `meta` if the result is unexpected.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta={'x': 'f8', 'y': 'f8'}) for dataframe result\n",
      "  or:     .apply(func, meta=('x', 'f8'))            for series result\n",
      "  dd_pz = dd_px.groupby('ticker', group_keys=False).apply(lambda x: x.sort_values(\"Date\"))\n",
      "C:\\Users\\The Winner\\AppData\\Local\\Temp\\ipykernel_11120\\159928415.py:8: UserWarning: `meta` is not specified, inferred from partial data. Please provide `meta` if the result is unexpected.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta={'x': 'f8', 'y': 'f8'}) for dataframe result\n",
      "  or:     .apply(func, meta=('x', 'f8'))            for series result\n",
      "  dd_shift = dd_pz.groupby('ticker', group_keys=False).apply(\n",
      "C:\\Users\\The Winner\\AppData\\Local\\Temp\\ipykernel_11120\\159928415.py:13: UserWarning: `meta` is not specified, inferred from partial data. Please provide `meta` if the result is unexpected.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta={'x': 'f8', 'y': 'f8'}) for dataframe result\n",
      "  or:     .apply(func, meta=('x', 'f8'))            for series result\n",
      "  dd_rets = dd_shift.groupby('ticker', group_keys=False).apply(\n",
      "C:\\Users\\The Winner\\AppData\\Local\\Temp\\ipykernel_11120\\159928415.py:17: UserWarning: `meta` is not specified, inferred from partial data. Please provide `meta` if the result is unexpected.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta={'x': 'f8', 'y': 'f8'}) for dataframe result\n",
      "  or:     .apply(func, meta=('x', 'f8'))            for series result\n",
      "  dd_hi_lo_range = dd_rets.groupby('ticker', group_keys=False).apply(\n"
     ]
    }
   ],
   "source": [
    "# Read files\n",
    "dd_px = dd.read_parquet(parquet_files).set_index(\"ticker\")\n",
    "# group values by ticker, sort values by date\n",
    "dd_pz = dd_px.groupby('ticker', group_keys=False).apply(lambda x: x.sort_values(\"Date\"))\n",
    "\n",
    "# Add lags for variables Close and Adj_Close.\n",
    "#dd_px.columns # Index(['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'source', 'Year'], dtype='object')\n",
    "dd_shift = dd_pz.groupby('ticker', group_keys=False).apply(\n",
    "    lambda x: x.assign(Close_lag_1 = x['Close'].shift(1),\n",
    "                       Adj_Close_lag_1 = x['Adj Close'].shift(1))\n",
    ")\n",
    "# Add returns based on Close:          `returns`: (Close / Close_lag_1) - 1\n",
    "dd_rets = dd_shift.groupby('ticker', group_keys=False).apply(\n",
    "    lambda x: x.assign(returns = x['Close']/x['Close_lag_1'] - 1)\n",
    ")\n",
    "# Add the following range: `hi_lo_range`: this is the day's High minus Low\n",
    "dd_hi_lo_range = dd_rets.groupby('ticker', group_keys=False).apply(\n",
    "    lambda x: x.assign(hi_lo_range = x['High'] - x['Low'])\n",
    ")\n",
    "# Assign the result to `dd_feat`\n",
    "dd_feat = dd_hi_lo_range\n",
    "\n",
    "#dd_feat\n",
    "#dd_feat.compute()\n",
    "#type(dd_feat.compute())\n",
    "#len(dd_feat) # 359088\n",
    "#len(dd_feat.columns) #13\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Convert the Dask data frame to a pandas data frame. \n",
    "+ Add a new feature containing the moving average of `returns` using a window of 10 days. There are several ways to solve this task, a simple one uses `.rolling(10).mean()`.\n",
    "\n",
    "(3 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\The Winner\\AppData\\Local\\Temp\\ipykernel_11120\\601101431.py:4: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  pd_feat= pd_feat.groupby('ticker').apply(\n"
     ]
    }
   ],
   "source": [
    "# Convert the Dask data frame to a pandas data frame\n",
    "pd_feat = dd_feat.compute().reset_index()\n",
    "# Add a new feature containing the moving average of `returns` using a window of 10 days. There are several ways to solve this task, a simple one uses `.rolling(10).mean()`\n",
    "pd_feat= pd_feat.groupby('ticker').apply(\n",
    "    lambda x: x.assign(mov_avg = x['returns'].rolling(10).mean())\n",
    ")\n",
    "# remove extra index created\n",
    "pd_feat.reset_index(drop=True, inplace=True)\n",
    "#pd_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please comment:\n",
    "\n",
    "+ Was it necessary to convert to pandas to calculate the moving average return?\n",
    "\n",
    "No, we could have done it with Dask\n",
    "+ Would it have been better to do it in Dask? Why?\n",
    "\n",
    "In Dask, there is bigger overhead as it does paralelization and prepares the processes, but then it calculates it faster.\n",
    "\n",
    "It is a trade off between overhead time vs calculation saved time.\n",
    "\n",
    "This size, likely it would have been better to use Dask. \n",
    "\n",
    "See below code to do the moving average feature. In my computer, doing it with Pandas takes 16 seconds, doing it with Dask shows 0.0 seconds if not computed. \n",
    "\n",
    "It doesn't surprise me, in the past I was encountering 100x performance improvements with Numpy arrays instead of Pandas dataframes.\n",
    "\n",
    "(1 pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\The Winner\\AppData\\Local\\Temp\\ipykernel_2352\\3233744760.py:2: UserWarning: `meta` is not specified, inferred from partial data. Please provide `meta` if the result is unexpected.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta={'x': 'f8', 'y': 'f8'}) for dataframe result\n",
      "  or:     .apply(func, meta=('x', 'f8'))            for series result\n",
      "  dd_feat_mov_avg = dd_feat.groupby('ticker', group_keys=False).apply(\n"
     ]
    }
   ],
   "source": [
    "# In Dask, add a new feature containing the moving average of `returns` using a window of 10 days. There are several ways to solve this task, a simple one uses `.rolling(10).mean()`\n",
    "dd_feat_mov_avg = dd_feat.groupby('ticker', group_keys=False).apply(\n",
    "    lambda x: x.assign(mov_avg = x['returns'].rolling(10).mean())\n",
    ")\n",
    "\n",
    "#dd_feat_mov_avg.reset_index().compute()\n",
    "\n",
    "# https://docs.dask.org/en/stable/generated/dask.dataframe.DataFrame.reset_index.html\n",
    "# Note that unlike in pandas, the reset index for a Dask DataFrame will not be monotonically increasing from 0. \n",
    "# Instead, it will restart at 0 for each partition (e.g. index1 = [0, ..., 10], index2 = [0, ...]). \n",
    "# This is due to the inability to statically know the full length of the index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Note aside:\n",
    "\n",
    "After submission, in a branch named sandbox, I have picked a specific calculation, in this case the simple non-sense adding all the moving average column values from all tickers together, to find differences between methods. Things I learned:\n",
    "\n",
    "- if we sort the original values (after groupby by ticker) by date, we obtain different result. Said that, I should have sorted the values by date inside and before each groupby calculation\n",
    "\n",
    "- if we put together all the feature calculations in one single function and use map_partition, give different result, basically ensures that there will be 89 partitions (one per ticker) and that a ticker will not be split between different partitions, compromising the continuity of some calculations. Said that, I should have used map_partition\n",
    "\n",
    "- if we use the one function and map_partition as described above, and we add the calculation of the feature moving_average inside such function, we have everything in Dask, no converting to Pandas, gives also different result. In here, the results sorted by dates and not sorted by dates are very similar. Because the issue with partitions, I assume this is the cleanest and best result.\n",
    "\n",
    "- regarding calculation times, last option of all in Dask takes (in my computer, 4 threads) around 33 seconds (where Dask computes to give me only one value) vs around same 33 seconds of Dask+Pandas (where Dask computes all dataframe values to give it to Pandas). With this size of data there is no difference. \n",
    "\n",
    "- if the 3 different options are inside same Jupiter Notebook and click Run All, times get distorted and also results. Even if I use different variables name to avoid that, it reuses some calculations (I did not dig deep). When Restart, run one option, Restart, run second option, etc..., it gives always same results.\n",
    "\n",
    "- the three different options (this assignement, using map_partions + pandas, using map_partitions Dask only), when we sort values by date grouped by ticker at the beginning, all three (3) give exact same result. When donig this sorting, we obtain three (3) different results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criteria\n",
    "\n",
    "The [rubric](./assignment_1_rubric_clean.xlsx) contains the criteria for grading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Information\n",
    "\n",
    "🚨 **Please review our [Assignment Submission Guide](https://github.com/UofT-DSI/onboarding/blob/main/onboarding_documents/submissions.md)** 🚨 for detailed instructions on how to format, branch, and submit your work. Following these guidelines is crucial for your submissions to be evaluated correctly.\n",
    "\n",
    "### Submission Parameters:\n",
    "* Submission Due Date: `HH:MM AM/PM - DD/MM/YYYY`\n",
    "* The branch name for your repo should be: `assignment-1`\n",
    "* What to submit for this assignment:\n",
    "    * This Jupyter Notebook (assignment_1.ipynb) should be populated and should be the only change in your pull request.\n",
    "* What the pull request link should look like for this assignment: `https://github.com/<your_github_username>/production/pull/<pr_id>`\n",
    "    * Open a private window in your browser. Copy and paste the link to your pull request into the address bar. Make sure you can see your pull request properly. This helps the technical facilitator and learning support staff review your submission easily.\n",
    "\n",
    "Checklist:\n",
    "- [ ] Created a branch with the correct naming convention.\n",
    "- [ ] Ensured that the repository is public.\n",
    "- [ ] Reviewed the PR description guidelines and adhered to them.\n",
    "- [ ] Verify that the link is accessible in a private browser window.\n",
    "\n",
    "If you encounter any difficulties or have questions, please don't hesitate to reach out to our team via our Slack at `#cohort-3-help`. Our Technical Facilitators and Learning Support staff are here to help you navigate any challenges."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsi_participant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
